{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa26ac2f-483c-4deb-b6a0-6ffcffa6b615",
   "metadata": {},
   "source": [
    "# **A Comprehensive Guide on Deep Learning Optimizers**\n",
    "\n",
    "### Overview\n",
    "Deep learning is the subfield of machine learning which is used to perform complex tasks such as speech recognition, text classification, etc. A deep learning model consists of activation function, input, output, hidden layers, loss function, etc. Any deep learning model tries to generalize the data using an algorithm and tries to make predictions on the unseen data. We need an algorithm that maps the examples of inputs to that of the outputs and an optimization algorithm. An optimization algorithm finds the value of the parameters(weights) that minimize the error when mapping inputs to outputs. These optimization algorithms or optimizers widely affect the accuracy of the deep learning model. They as well as affect the speed training of the model. But first of all the question arises what an optimizer really is?\n",
    "\n",
    "While training the deep learning model, we need to modify each epoch’s weights and minimize the loss function. An optimizer is a function or an algorithm that modifies the attributes of the neural network, such as weights and learning rate. Thus, it helps in reducing the overall loss and improve the accuracy. The problem of choosing the right weights for the model is a daunting task, as a deep learning model generally consists of millions of parameters. It raises the need to choose a suitable optimization algorithm for your application. Hence understanding these algorithms is necessary before having a deep dive into the field.\n",
    "\n",
    "You can use different optimizers to make changes in your weights and learning rate. However, choosing the best optimizer depends upon the application. As a beginner, one evil thought that comes to mind is that we try all the possibilities and choose the one that shows the best results. This might not be a problem initially, but when dealing with hundreds of gigabytes of data, even a single epoch can take a considerable amount of time. So randomly choosing an algorithm is no less than gambling with your precious time that you will realize sooner or later in your journey.\n",
    "\n",
    "In this guide, we will learn about different optimizers used in building a deep learning model and the factors that could make you choose an optimizer instead of others for your application.\n",
    "\n",
    "We are going to learn the following optimizers in this guide and look at the pros and cons of each algorithm. So that, at the end of the article, you will be able to compare various optimizers and the procedure they are based upon.\n",
    "\n",
    "1. **Gradient Descent**\n",
    "2. **Stochastic Gradient Descent**\n",
    "3. **Stochastic Gradient descent with momentum**\n",
    "4. **Mini-Batch Gradient Descent**\n",
    "5. **Adagrad**\n",
    "6. **RMSProp**\n",
    "7. **AdaDelta**\n",
    "8. **Adam**\n",
    "\n",
    "**Before proceeding there are few terms that you should be familiar with.**\n",
    "\n",
    "**Epoch** – The number of times the algorithm runs on the whole training dataset.\n",
    "\n",
    "**Sample** – A single row of a dataset.\n",
    "\n",
    "**Batch** – It denotes the number of samples to be taken to for updating the model parameters.\n",
    "\n",
    "**Learning rate** – It is a parameter that provides the model a scale of how much model weights should be updated.\n",
    "\n",
    "**Cost Function/Loss Function** – A cost function is used to calculate the cost that is the difference between the predicted value and the actual value.\n",
    "\n",
    "**Weights/ Bias** – The learnable parameters in a model that controls the signal between two neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432ae6e-c0f1-47b0-b745-d422bd34d179",
   "metadata": {},
   "source": [
    "## 01. Gradient Descent Deep Learning Optimizer\n",
    "Gradient Descent can be considered as the popular kid among the class of optimizers. This optimization algorithm uses calculus to modify the values consistently and to achieve the local minimum. Before moving ahead, you might have the question of what a gradient is?\n",
    "\n",
    "In simple terms, consider you are holding a ball resting at the top of a bowl. When you lose the ball, it goes along the steepest direction and eventually settles at the bottom of the bowl. A Gradient provides the ball in the steepest direction to reach the local minimum that is the bottom of the bowl.\n",
    "\n",
    "So **Gradient Descent** is one of the optimizers which helps in calculating the new weights. Let’s understand step by step how Gradient Descent optimizes the cost function.\n",
    "\n",
    "In the image below, the curve is our cost function curve and our aim is the minimize the error such that Jmin i.e global minima is achieved.\n",
    "\n",
    "<img src =\"https://editor.analyticsvidhya.com/uploads/22880gd.png\" width = 500>\n",
    "\n",
    "**Steps to achieve the global minima:**\n",
    "\n",
    "1. First, **the weights are initialized randomly** i.e random value of the weight, and intercepts are assigned to the model while forward propagation and **the errors are calculated** after all the computation. (As discussed above)\n",
    "\n",
    "2. Then the **gradient is calculated i.e derivative of error w.r.t current weights**\n",
    "\n",
    "3. Then new weights are calculated using the below formula, where a is the learning rate which is the parameter also known as step size to control the speed or steps of the backpropagation. It gives additional control on how fast we want to move on the curve to reach global minima.\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/45538bp_update_formula.png\" width = 500>\n",
    "\n",
    "4. This process of calculating the new weights, then errors from the new weights, and then updation of weights continues till we reach global minima and loss is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f6b41-b07e-4304-aada-0e86c941415a",
   "metadata": {},
   "source": [
    "## 02. Stochastic Gradient Descent Deep Learning Optimizer\n",
    "This is another variant of the Gradient Descent optimizer with an additional capability of working with the data with a non-convex optimization problem. The problem with such data is that the cost function results to rest at the local minima which are not suitable for your learning algorithm.\n",
    "\n",
    "\n",
    "Rather than going for batch processing, this optimizer focuses on performing one update at a time. It is therefore usually much faster, also the cost function minimizes after each iteration (EPOCH). It performs frequent updates with a high variance that causes the objective function(cost function) to fluctuate heavily. Due to which it makes the gradient to jump to a potential Global Minima.\n",
    "\n",
    "\n",
    "However, if we choose a learning rate that is too small, it may lead to very slow convergence, while a larger learning rate can make it difficult to converge and cause the cost function to fluctuate around the minimum or even to diverge away from the global minima.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/710/1*a8J4tdhbxlkiIGpDALtIsw.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b855f-5dbd-402f-b429-9aeb4d612667",
   "metadata": {},
   "source": [
    "# 03. Mini Batch Gradient Descent\n",
    "In this variant of gradient descent instead of taking all the training data, only a subset of the dataset is used for calculating the loss function. Since we are using a batch of data instead of taking the whole dataset, fewer iterations are needed. That is why the mini-batch gradient descent algorithm is faster than both stochastic gradient descent and batch gradient descent algorithms. This algorithm is more efficient and robust than the earlier variants of gradient descent. As the algorithm uses batching, all the training data need not be loaded in the memory, thus making the process more efficient to implement. Moreover, the cost function in mini-batch gradient descent is noisier than the batch gradient descent algorithm but smoother than that of the stochastic gradient descent algorithm. Because of this, mini-batch gradient descent is ideal and provides a good balance between speed and accuracy.\n",
    "\n",
    "Despite, all that, the mini-batch gradient descent algorithm has some downsides too. It needs a hyperparameter that is “mini-batch-size”, which needs to be tuned to achieve the required accuracy. Although, the batch size of 32 is considered to be appropriate for almost every case. Also, in some cases, it results in poor final accuracy. Due to this, there needs a rise to look for other alternatives too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360d5b3-4824-44b6-abe8-e5cd996d518e",
   "metadata": {},
   "source": [
    "Other Sources\n",
    "\n",
    "[1. analyticsvidhya](https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/)\n",
    "\n",
    "[2. medium](https://medium.com/analytics-vidhya/this-blog-post-aims-at-explaining-the-behavior-of-different-algorithms-for-optimizing-gradient-46159a97a8c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830bbe7-4dd5-4dad-89bb-4f49221bfa17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
